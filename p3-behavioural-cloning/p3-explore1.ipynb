{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout2D, ELU\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  14112\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "reduced = False\n",
    "\n",
    "if reduced == True:\n",
    "    csv_filepath = 'data-udacity/driving_log_reduced.csv'\n",
    "else:\n",
    "    csv_filepath = 'data-udacity/driving_log.csv'\n",
    "samples = []\n",
    "with open(csv_filepath) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "def add_to_samples(csv_filepath, samples):\n",
    "    with open(csv_filepath) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            samples.append(line)\n",
    "    return samples\n",
    "\n",
    "samples = add_to_samples('data-extra-laps/driving_log.csv', samples)\n",
    "\n",
    "samples = add_to_samples('data-recovery/driving_log.csv', samples)\n",
    "\n",
    "\"\"\"\n",
    "samples = add_to_samples('data-udacity/driving_log_18_bridge_reduced2.csv)\n",
    "samples = add_to_samples('data-udacity-flipped/driving_csv_flipped_headoff.csv)\n",
    "\"\"\"\n",
    "samples = samples[1:]\n",
    "print(\"Samples: \", len(samples))    \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = './data-udacity/'+batch_sample[0]\n",
    "                # name = './data-udacity/IMG/'+batch_sample[0].split('/')[-1]\n",
    "                center_image = mpimg.imread(name)\n",
    "                center_angle = float(batch_sample[3])\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            # print(\"X_train: \", X_train)\n",
    "            # print(\"y_train: \", y_train)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize(image):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 66, 200)\n",
    "\n",
    "def resize_comma(image):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 40, 160)\n",
    "\n",
    "def resize_comma_10_40(image):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 40, 160)\n",
    "\n",
    "def halve_size(image):\n",
    "    imshape = image.shape\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, imshape[0] // 2, imshape[1] // 2)\n",
    "\n",
    "def resize_80_160(image):\n",
    "    imshape = image.shape\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 80, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "cropping2d_2 (Cropping2D)        (None, 65, 320, 3)    0           cropping2d_input_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 40, 160, 3)    0           cropping2d_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, 40, 160, 1)    0           lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)                (None, 40, 160, 1)    0           lambda_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 10, 40, 16)    1040        lambda_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_5 (ELU)                      (None, 10, 40, 16)    0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 5, 20, 32)     12832       elu_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_6 (ELU)                      (None, 5, 20, 32)     0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 3, 10, 64)     51264       elu_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1920)          0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "elu_7 (ELU)                      (None, 1920)          0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           983552      elu_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_8 (ELU)                      (None, 512)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             513         elu_8[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1049201\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Comma.ai model\n",
    "# https://github.com/commaai/research/blob/master/train_steering_model.py\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Crop 50 pixels from the top of the image and 20 from the bottom\n",
    "model.add(Cropping2D(cropping=((70, 25), (0, 0)),\n",
    "                     dim_ordering='tf', # default\n",
    "                     input_shape=(160, 320, 3)))\n",
    "\n",
    "# Resize the data\n",
    "model.add(Lambda(resize_comma))\n",
    "\n",
    "model.add(Lambda(lambda x: x[:,:,:,0:1]))\n",
    "\n",
    "model.add(Lambda(lambda x: (x/255.0) - 0.5))\n",
    "\n",
    "model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(.2))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(512))\n",
    "# model.add(Dropout(.5))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "\n",
    "model.compile(optimizer=adam, loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def rgb_to_hsv(image):\n",
    "    # import cv2 \n",
    "    # cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    import tensorflow as tf\n",
    "    return tf.image.rgb_to_hsv(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing reference: [Geoff Breemer](https://carnd-forums.udacity.com/questions/36045049/answers/36047341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.5410Epoch 00000: saving model to ./tmp/comma-v2f.00-0.02.hdf5\n",
      "7232/7232 [==============================] - 35s - loss: 0.0172 - acc: 0.5415 - val_loss: 0.0157 - val_acc: 0.5349\n",
      "Epoch 2/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.5435Epoch 00001: saving model to ./tmp/comma-v2f.01-0.01.hdf5\n",
      "7232/7232 [==============================] - 38s - loss: 0.0176 - acc: 0.5431 - val_loss: 0.0150 - val_acc: 0.5557\n",
      "Epoch 3/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.5410Epoch 00002: saving model to ./tmp/comma-v2f.02-0.02.hdf5\n",
      "7232/7232 [==============================] - 46s - loss: 0.0169 - acc: 0.5415 - val_loss: 0.0165 - val_acc: 0.5264\n",
      "Epoch 4/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.5435Epoch 00003: saving model to ./tmp/comma-v2f.03-0.01.hdf5\n",
      "7232/7232 [==============================] - 47s - loss: 0.0174 - acc: 0.5431 - val_loss: 0.0144 - val_acc: 0.5569\n",
      "Epoch 5/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.5410Epoch 00004: saving model to ./tmp/comma-v2f.04-0.02.hdf5\n",
      "7232/7232 [==============================] - 43s - loss: 0.0168 - acc: 0.5415 - val_loss: 0.0159 - val_acc: 0.5322\n",
      "Epoch 6/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.5435Epoch 00005: saving model to ./tmp/comma-v2f.05-0.01.hdf5\n",
      "7232/7232 [==============================] - 48s - loss: 0.0174 - acc: 0.5431 - val_loss: 0.0144 - val_acc: 0.5661\n",
      "Epoch 7/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.5410Epoch 00006: saving model to ./tmp/comma-v2f.06-0.02.hdf5\n",
      "7232/7232 [==============================] - 47s - loss: 0.0168 - acc: 0.5415 - val_loss: 0.0156 - val_acc: 0.5248\n",
      "Epoch 8/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.5435Epoch 00007: saving model to ./tmp/comma-v2f.07-0.01.hdf5\n",
      "7232/7232 [==============================] - 52s - loss: 0.0173 - acc: 0.5431 - val_loss: 0.0140 - val_acc: 0.5721\n",
      "Epoch 9/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.5410Epoch 00008: saving model to ./tmp/comma-v2f.08-0.02.hdf5\n",
      "7232/7232 [==============================] - 47s - loss: 0.0168 - acc: 0.5415 - val_loss: 0.0161 - val_acc: 0.5149\n",
      "Epoch 10/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.5435Epoch 00009: saving model to ./tmp/comma-v2f.09-0.01.hdf5\n",
      "7232/7232 [==============================] - 46s - loss: 0.0173 - acc: 0.5431 - val_loss: 0.0141 - val_acc: 0.5685\n",
      "Epoch 11/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.5410Epoch 00010: saving model to ./tmp/comma-v2f.10-0.02.hdf5\n",
      "7232/7232 [==============================] - 38s - loss: 0.0167 - acc: 0.5415 - val_loss: 0.0163 - val_acc: 0.5186\n",
      "Epoch 12/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.5435Epoch 00011: saving model to ./tmp/comma-v2f.11-0.01.hdf5\n",
      "7232/7232 [==============================] - 34s - loss: 0.0173 - acc: 0.5431 - val_loss: 0.0142 - val_acc: 0.5661\n",
      "Epoch 13/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.5410Epoch 00012: saving model to ./tmp/comma-v2f.12-0.02.hdf5\n",
      "7232/7232 [==============================] - 31s - loss: 0.0167 - acc: 0.5415 - val_loss: 0.0150 - val_acc: 0.5384\n",
      "Epoch 14/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.5435Epoch 00013: saving model to ./tmp/comma-v2f.13-0.02.hdf5\n",
      "7232/7232 [==============================] - 32s - loss: 0.0173 - acc: 0.5431 - val_loss: 0.0154 - val_acc: 0.5505\n",
      "Epoch 15/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.5410Epoch 00014: saving model to ./tmp/comma-v2f.14-0.01.hdf5\n",
      "7232/7232 [==============================] - 32s - loss: 0.0167 - acc: 0.5415 - val_loss: 0.0141 - val_acc: 0.5532\n",
      "Epoch 16/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.5435Epoch 00015: saving model to ./tmp/comma-v2f.15-0.02.hdf5\n",
      "7232/7232 [==============================] - 32s - loss: 0.0172 - acc: 0.5431 - val_loss: 0.0156 - val_acc: 0.5373\n",
      "Epoch 17/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.5410Epoch 00016: saving model to ./tmp/comma-v2f.16-0.01.hdf5\n",
      "7232/7232 [==============================] - 33s - loss: 0.0167 - acc: 0.5415 - val_loss: 0.0147 - val_acc: 0.5582\n",
      "Epoch 18/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.5435Epoch 00017: saving model to ./tmp/comma-v2f.17-0.02.hdf5\n",
      "7232/7232 [==============================] - 37s - loss: 0.0172 - acc: 0.5431 - val_loss: 0.0156 - val_acc: 0.5349\n",
      "Epoch 19/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.5410Epoch 00018: saving model to ./tmp/comma-v2f.18-0.01.hdf5\n",
      "7232/7232 [==============================] - 31s - loss: 0.0167 - acc: 0.5415 - val_loss: 0.0142 - val_acc: 0.5557\n",
      "Epoch 20/20\n",
      "7200/7232 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.5435Epoch 00019: saving model to ./tmp/comma-v2f.19-0.02.hdf5\n",
      "7232/7232 [==============================] - 30s - loss: 0.0172 - acc: 0.5431 - val_loss: 0.0165 - val_acc: 0.5264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12143a518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 32\n",
    "nb_epoch = 20\n",
    "# data_augmentation = True\n",
    " \n",
    "checkpointer = ModelCheckpoint(filepath=\"./tmp/comma-v2f.{epoch:02d}-{val_loss:.2f}.hdf5\", verbose=1, save_best_only=False)\n",
    "    \n",
    "model.fit_generator(train_generator, \n",
    "                    samples_per_epoch=len(train_samples), \n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=len(validation_samples), nb_epoch=nb_epoch,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model-comma-v2f.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create reduced dataset\n",
    "\n",
    "zero = driving_csv[driving_csv[\"steering\"] == 0]\n",
    "print(\"Samples taken out: \", len(zero) // 2)\n",
    "zeros_half = zero.sample(frac=0.5)\n",
    "nonzero = driving_csv[driving_csv[\"steering\"] != 0]\n",
    "reduced_dataset = pd.concat([zeros_half, nonzero])\n",
    "reduced_dataset.to_csv(\"data-udacity/driving_log_reduced.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Flip all of Udacity's data\n",
    "X_path = [driving_csv.iloc[i][\"center\"] \\\n",
    "              for i in range(len(driving_csv))]\n",
    "for filepath in X_path:\n",
    "    cv2.imread('data-udacity/' + filepath)\n",
    "    flipped_image = cv2.flip(input_image,1)\n",
    "    cv2.imwrite('data-udacity-flipped/' + filepath, flipped_image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Option D: reduced Udacity's dataset\n",
    "\n",
    "## Import data\n",
    "# Added header row manually to CSV.\n",
    "driving_csv = pd.read_csv(\"data-udacity/driving_log.csv\")\n",
    "\n",
    "# Examine data\n",
    "print(\"Number of datapoints: %d\" % len(driving_csv))\n",
    "driving_csv.head()\n",
    "\n",
    "greater_half = driving_csv[driving_csv[\"steering\"] > 0.5]\n",
    "greater_half_neg = driving_csv[driving_csv[\"steering\"] < -0.5]\n",
    "\n",
    "smaller_dataset = pd.concat([greater_half, greater_half_neg])\n",
    "\n",
    "i = -0.5\n",
    "while i < 0.5:\n",
    "    smaller_dataset = pd.concat([smaller_dataset, \n",
    "              driving_csv[driving_csv[\"steering\"] > i].sort_values(by=\"steering\").iloc[0:3]])\n",
    "    i += 0.05\n",
    "print(len(smaller_dataset))\n",
    "\n",
    "# Extract centre image and steering angle from table\n",
    "# Format: X_path: centre image name, y: steeringa angle\n",
    "X_path = [smaller_dataset.iloc[i][\"center\"] \\\n",
    "              for i in range(len(smaller_dataset))]\n",
    "y = [smaller_dataset.iloc[i][\"steering\"] \\\n",
    "              for i in range(len(smaller_dataset))]\n",
    "\n",
    "# Import images\n",
    "X_images = [mpimg.imread(\"data-udacity/\" + image_path) for image_path in X_path]\n",
    "\n",
    "# View image\n",
    "print(\"Images: %d\" % len(X_images))\n",
    "print(\"Sample image\")\n",
    "plt.imshow(X_images[0])\n",
    "# X_images[0]\n",
    "\n",
    "imshape = X_images[0].shape\n",
    "print(\"Image shape: \", imshape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train model\n",
    "batch_size = 5\n",
    "nb_epoch = 30\n",
    "# data_augmentation = True\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              # validation_split=0.1,\n",
    "              validation_data=(X_val, y_val),\n",
    "              # shuffle=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def flip(image):\n",
    "    import tensorflow as tf\n",
    "    return tf.reverse_sequence(image, [200] * 66, 1, batch_dim=0)\n",
    "\n",
    "def flip_tf(image):\n",
    "    import tensorflow as tf\n",
    "    return tf.image.flip_left_right(image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NVIDIA End to End Learning Pipeline Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Crop the images\n",
    "model.add(Cropping2D(cropping=((60, 20), (0, 0)),\n",
    "                     dim_ordering='tf', # default\n",
    "                     input_shape=(160, 320, 3)))\n",
    "\n",
    "# Resize the data\n",
    "model.add(Lambda(resize))\n",
    "\n",
    "model.add(Lambda(lambda x: x[:,:,:,0:1]))\n",
    "\n",
    "# Normalise data\n",
    "# TODO: some people use /255.0 - 0.5. Why?\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.))\n",
    "\n",
    "\n",
    "# Conv layer 1, 5x5 kernel to 24@ (from 3@)\n",
    "# TODO: What does the number of filters MEAN?\n",
    "# Stride of 2x2\n",
    "model.add(Convolution2D(24, 36, 104,\n",
    "                        input_shape=(66,200))\n",
    "         )\n",
    "\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 2, 5x5 kernel to 36@\n",
    "model.add(Convolution2D(36, 18, 50))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 3, 5x5 kernel to 48@\n",
    "model.add(Convolution2D(48, 10, 32))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 4, 3x3 kernel to 64@\n",
    "model.add(Convolution2D(64, 3, 12))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 5, 3x3 kernel to 64@\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Removed: fully connected layer 1, 1164 neurons\n",
    "\n",
    "# Fc2, 100 neurons\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Fc3, 50 neurons\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Fc4, 10 neurons\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Output\n",
    "model.add(Dense(1, activation=\"tanh\"))\n",
    "\n",
    "# Compile model\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
