{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Learning and Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn.py\n",
    "from miniflow import *\n",
    "\n",
    "x, y, z = Input(), Input(), Input()\n",
    "inputs = [x, y, z]\n",
    "\n",
    "weight_x, weight_y, weight_z = Input(), Input(), Input()\n",
    "weights = [weight_x, weight_y, weight_z]\n",
    "\n",
    "bias = Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "\tx: 6,\n",
    "\ty: 14,\n",
    "\tz: 3,\n",
    "\tweight_x: 0.5,\n",
    "\tweight_y: 0.25,\n",
    "\tweight_z: 1.4,\n",
    "\tbias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write the Linear#forward method below!\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, inbound_neurons=[]):\n",
    "        # Neurons from which this Node receives values\n",
    "        self.inbound_neurons = inbound_neurons\n",
    "        # Neurons to which this Node passes values\n",
    "        self.outbound_neurons = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_neurons:\n",
    "            n.outbound_neurons.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_neurons` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Neuron):\n",
    "    def __init__(self):\n",
    "        # An Input Neuron has no inbound neurons,\n",
    "        # so no need to pass anything to the Neuron instantiator\n",
    "        Neuron.__init__(self)\n",
    "\n",
    "        # NOTE: Input Neuron is the only Neuron where the value\n",
    "        # may be passed as an argument to forward().\n",
    "        #\n",
    "        # All other Neuron implementations should get the value\n",
    "        # of the previous neurons from self.inbound_neurons\n",
    "        #\n",
    "        # Example:\n",
    "        # val0 = self.inbound_neurons[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Linear(Neuron):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Neuron.__init__(self, inputs)\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "        \n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        self.inbound_neuron_values = [neuron.value for neuron in self.inbound_neurons]\n",
    "        print(\"self.inbound_neuron_values\", self.inbound_neuron_values)\n",
    "        print(\"self.weights\", self.weights)\n",
    "        self.weights_values = [weight.value for weight in self.weights]\n",
    "        self.value = np.dot(self.weights_values, self.inbound_neuron_values)\n",
    "        self.value += self.bias.value\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the neurons in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Neuron and the value is the respective value feed to that Neuron.\n",
    "\n",
    "    Returns a list of sorted neurons.\n",
    "    \"\"\"\n",
    "\n",
    "    input_neurons = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    neurons = [n for n in input_neurons]\n",
    "    while len(neurons) > 0:\n",
    "        n = neurons.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_neurons:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            neurons.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_neurons)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_neurons:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_Neuron, sorted_neurons):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted neurons.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_Neuron`: A Neuron in the graph, should be the output Neuron (have no outgoing edges).\n",
    "        `sorted_neurons`: a topologically sorted list of neurons.\n",
    "\n",
    "    Returns the output Neuron's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_neurons:\n",
    "        n.forward()\n",
    "\n",
    "    return output_Neuron.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Much more elegant:\n",
    "\n",
    "self.value = self.bias.value\n",
    "for w, x in zip(self.weights, self.inbound_neurons):\n",
    "    self.value += w.value * x.value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "('self.inbound_neuron_values', [6, 14, 3])\n",
    "('self.weights', [<miniflow.Input instance at 0x7efd68457950>, <miniflow.Input instance at 0x7efd68457998>, <miniflow.Input instance at 0x7efd68457a28>])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Neurons to Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to feed in multiple data examples in each forward pass rather than just 1. This is because the examples can be processed in parallel, resulting in big performance gains.\n",
    "Number of examples passed forward in each forward pass: batch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, inbound_layer, weights, bias):\n",
    "        # Notice the ordering of the input layers passed to the\n",
    "        # Layer constructor.\n",
    "        Layer.__init__(self, [inbound_layer, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this layer to the linear transform output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        inputs = self.inbound_layers[0].value\n",
    "        weights = self.inbound_layers[1].value\n",
    "        bias = self.inbound_layers[2].value\n",
    "        self.value = np.dot(inputs, weights) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "Activation function used to categorise the output.\n",
    "\n",
    "Linear -> Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self, layer):\n",
    "        Layer.__init__(self, [layer])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x)) # the `.` ensures that `1` is a float\n",
    "\n",
    "    def forward(self):\n",
    "        input_value = self.inbound_layers[0].value\n",
    "        self.value = self._sigmoid(input_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flattening matrices \n",
    "# TODO: Create Anki card\n",
    "\n",
    "# 2 by 2 matrices\n",
    "w1  = np.array([[1, 2], [3, 4]])\n",
    "w2  = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# flatten\n",
    "w1_flat = np.reshape(w1, -1)\n",
    "w2_flat = np.reshape(w2, -1)\n",
    "\n",
    "w = np.concatenate((w1_flat, w2_flat))\n",
    "# array([1, 2, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pretty sure MSE doesn't divide by two but it's nice for derivatives.\n",
    "\n",
    "def MSE(computed_output, ideal_output, n_inputs):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error.\n",
    "\n",
    "    `computed_output`: a numpy array\n",
    "    `ideal_output`: a numpy array\n",
    "    `n_inputs`: the number of inputs\n",
    "\n",
    "    Return the mean squared error of output layer.\n",
    "    \"\"\"\n",
    "    first = 1. / (2. * n_inputs)\n",
    "    norm = np.linalg.norm(ideal_output - computed_output)\n",
    "    return first * np.square(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Given that gradient points in the direction of steepest ascent, it follows that moving along the direction of the negative gradient creates the steepest descent. This fact forms the crux of gradient descent.\n",
    "\n",
    "To move in a negative direction, networks usually define a learning rate, a small negative number, to control the size of the step along the negative gradient that the cost function should move between each iteration through the network.\n",
    "\n",
    "When designing neural networks, you can tweak the learning rate as a parameter. The more negative the learning rate, the faster the network will change. Of course, bigger changes mean that your network may overshoot the minimum in the loss (and fail to ever land close to it!). The smaller the learning rate, the slower the network learns (and when neural networks take hours or days to run on cloud machines, time is money!).\n",
    "\n",
    "Learning in real neural networks strives to minimize the cost as much as possible. To see how, consider the cost function again.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
