{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robotics:\n",
    "1. Perception: Sense what is in the world (CV)\n",
    "    - Detect lane markings, vehicles.\n",
    "    - Alternative to cameras: radar + lidar (see the world in 3D, more costly, lower spatial resolution compared to cameras)\n",
    "        - Possible that in future cars are fitted with only a few cameras for perception as opposed to cameras, radars and lidars.\n",
    "2. Decide what to do based on that perception\n",
    "3. Perform action to carry out decision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two pieces:\n",
    "- Advanced lane lines\n",
    "- Vehicle (other vehicles) location and tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action: Steer car** \n",
    "- Steer car: Need to measure how much lane is curving. \n",
    "- Need to get perspective transformation from front view to birds-eye view.\n",
    "- Correct for effect of image distortion.\n",
    "    - changes shape and size of objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for image distortion\n",
    "\n",
    "### Types of distortion\n",
    "\n",
    "![](images/18-1.png)\n",
    "\n",
    "#### Pinhole camera model\n",
    "\n",
    "![](images/18-2.png)\n",
    "Image captured by pinhole camera is upside-down and reversed.\n",
    "\n",
    "Real cameras don't use pinholes - they use lenses. Lenses introduce distortion. Light rays bend too much or too little at the edges of a lens.\n",
    "\n",
    "![](images/18-4.png)\n",
    "\n",
    "\n",
    "**Radial distortion**: Real cameras use curved lenses to form an image, and light rays often bend a little too much or too little at the edges of these lenses. This creates an effect that distorts the edges of images, so that lines or objects appear more or less curved than they actually are. This is called radial distortion, and it’s the most common type of distortion.\n",
    "- Fisheye lenses use radial distortion for a stylistic effect.\n",
    "\n",
    "**Tangential distortion**: This occurs when a camera’s lens is not aligned perfectly parallel to the imaging plane, where the camera film or sensor is. This makes an image look tilted so that some objects appear farther away or closer than they actually are.\n",
    "\n",
    "\n",
    "### Distortion coefficients and correction\n",
    "Typically five coefficients\n",
    "\n",
    "![](images/18-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To undistort points, OpenCV calculates `r`, where \n",
    "$$r = ||(x_{corrected}, y_{corrected})-(x_c, y_c)||$$\n",
    "\n",
    "where $(x_c, y_c)$ is the *distortion center*, the center of the image distortion.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](images/18-5.png)\n",
    "*Points in an distorted and undistorted (corrected) image. The point (x, y) is a single point in a distorted image and (x_corrected, y_corrected) is where that point will appear in the undistorted (corrected) image.*\n",
    "\n",
    "**Radial distortion correction formula**: We can calculate `(x_corrected, y_corrected)` thus:\n",
    "![](images/18-6.png)\n",
    "*Radial distortion correction.*\n",
    "\n",
    "**Tangential distortion correction formula**:\n",
    "![](images/18-7.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrating our camera\n",
    "\n",
    "1. Take pictures of known shapes to calibrate our camera. E.g. a chessboard (a regular, high-contrast pattern).\n",
    "    * Recommended to use at least 20 images for calibration. Images taken at different angles and distances.\n",
    "    * Include a test image.\n",
    "\n",
    "2. Store the object and image points from each image.\n",
    "    * Object points: 3D points in real world space\n",
    "        * E.g. for a 7 row by 9 column chessboard:\n",
    "            * ![](images/18-9.png)\n",
    "        * Initialise arrays and fill arrays using objpo = np.zeros\n",
    "            `objp = np.zeros((6*8,3), np.float32)\n",
    "            objp[:,:2] = np.mgrid(8:8,0:6].T.reshape[-1,2]`\n",
    "    * Image points: 2D points in image plane\n",
    "3. Find chessboard corners\n",
    "\n",
    "2. Create a transform that maps distorted points to undistorted (corrected) points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Corners\n",
    "\n",
    "Use the OpenCV functions [findChessboardCorners()](http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.findChessboardCorners) and [drawChessboardCorners()](http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.drawChessboardCorners) to automatically find and draw corners in your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding Corners exercise\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import glob\n",
    "# prepare object points\n",
    "nx = 8 # Number of inside corners in any given row\n",
    "ny = 6 # Number of inside corners in any given column\n",
    "\n",
    "# Read in and make a list of calibration images\n",
    "\n",
    "# glob allows us to read in files with consistent file names\n",
    "# e.g. calibration-1.jpg, calibration-2.jpg...\n",
    "images = glob.glob(\"calibration_images/calibration*.jpg\")\n",
    "\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    # Parameters: (image, chessboard dims, param for any flags)\n",
    "    # chessboard dims = inside corners, not squares.\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "    # If found, draw corners\n",
    "    if ret == True:\n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calibration image provided with result for Finding Corners exercise:*\n",
    "![](images/18-8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV functions to calibrate our camera\n",
    "\n",
    "`cv2.calibrateCamera`:\n",
    "![](images/18-10.png)\n",
    "\n",
    "* `gray.shape[::-1]`: Shape of the image\n",
    "* `dist`: distortion coefficient\n",
    "* `mtx`: Camera matrix\n",
    "* `rvecs, tvecs`: position of camera in the world.\n",
    "\n",
    "![](images/18-4.png)\n",
    "*Camera matrix*\n",
    "\n",
    "`cv2.undistort(img, mtx, dist, None, mtx)`:\n",
    "\n",
    "* `img`: image\n",
    "* `mtx`: Camera matrix\n",
    "* `dist`: distortion coefficients\n",
    "* Returns undistorted (destination) image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just copying from 'Calibrating Your Camera'\n",
    "\n",
    "Examples of Useful Code\n",
    "\n",
    "Converting an image, imported by cv2 or the glob API, to grayscale:\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "Note: If you are reading in an image using mpimg.imread() this will read in an RGB image and you should convert to grayscale using cv2.COLOR_RGB2GRAY, but if you are using cv2.imread() or the glob API, as happens in this video example, this will read in a BGR image and you should convert to grayscale using cv2.COLOR_BGR2GRAY. We'll learn more about color conversions later on in this lesson, but please keep this in mind as you write your own code and look at code examples.\n",
    "\n",
    "Finding chessboard corners (for an 8x6 board):\n",
    "\n",
    "```\n",
    "ret, corners = cv2.findChessboardCorners(gray, (8,6),None)```\n",
    "Drawing detected corners on an image:\n",
    "\n",
    "```\n",
    "img = cv2.drawChessboardCorners(img, (8,6), corners, ret)```\n",
    "Camera calibration, given object points, image points, and the shape of the grayscale image:\n",
    "\n",
    "```\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)```\n",
    "Undistorting a test image:\n",
    "\n",
    "```\n",
    "dst = cv2.undistort(img, mtx, dist, None, mtx)```\n",
    "A note on image shape\n",
    "The shape of the image, which is passed into the calibrateCamera function, is just the height and width of the image. One way to retrieve these values is by retrieving them from the grayscale image shape array gray.shape[::-1]. This returns the image height and width in pixel values like (960, 1280).\n",
    "\n",
    "Another way to retrieve the image shape, is to get them directly from the color image by retrieving the first two values in the color image shape array using img.shape[0:2]. This code snippet asks for just the first two values in the shape array.\n",
    "\n",
    "It's important to use an entire grayscale image shape or the first two values of a color image shape. This is because the entire shape of a color image will include a third value -- the number of color channels -- in addition to the height and width of the image. For example the shape array of a color image might be (960, 1280, 3), which are the pixel height and width of an image (960, 1280) and a third value (3) that represents the three color channels in the color image which you'll learn more about later, and if you try to pass these three values into the calibrateCamera function, you'll get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Read in the saved objpoints and imgpoints\n",
    "dist_pickle = pickle.load( open( \"wide_dist_pickle.p\", \"rb\" ) )\n",
    "objpoints = dist_pickle[\"objpoints\"]\n",
    "imgpoints = dist_pickle[\"imgpoints\"]\n",
    "\n",
    "# Read in an image\n",
    "img = cv2.imread('test_image.png')\n",
    "\n",
    "# TODO: Write a function that takes an image, object points, and image points\n",
    "# performs the camera calibration, image distortion correction and \n",
    "# returns the undistorted image\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    # Use cv2.calibrateCamera and cv2.undistort()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[0:2], None, None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "\n",
    "undistorted = cal_undistort(img, objpoints, imgpoints)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(undistorted)\n",
    "ax2.set_title('Undistorted Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting information from images of the road\n",
    "\n",
    "## Lane Curvature\n",
    "Why: Need to be told the correct steering angle to turn\n",
    "\n",
    "Process:\n",
    "1. Detect lane lines using masking and thresholding techniques.\n",
    "    * ![](images/18-11.png)\n",
    "2. Perform perspective transform to get birds' eye view of the lane.\n",
    "    * ![](images/18-12.png)\n",
    "\n",
    "3. Fit polynomial to lane line `f(y) = Ay^2 + By + C`.\n",
    "4. Extract curvature of lane lines using a mapping.\n",
    "    * ![](images/18-13.png)\n",
    "\n",
    "    * A gives you the curvature of the lane line, B gives you the heading or direction that the line is pointing, and C gives you the position of the line based on how far away it is from the very left of an image (y = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective transform\n",
    "Changes apparent persective. Maps the points in a given image to different, desired, image points with a new perspective.\n",
    "![](images/18-16.png)\n",
    "\n",
    "Examples of perspective:\n",
    "![](images/18-14.png)\n",
    "![](images/18-15.png)\n",
    "\n",
    "Birds eye view also allows us to map a car's location to e.g. Google Maps.\n",
    "\n",
    "![](images/18-17.png)\n",
    "![](images/18-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use traffic sign because it's easy to see if you've performed a perspective transform (well) when using a flat surface with distinct reference points e.g. text.\n",
    "Four points are enough to define a linear perspective transform. (Q: Why not three if it's a plane?)\n",
    "\n",
    "* Source coordinates: Manually find coordinates of points in an interactive window (mouse over points in the image).\n",
    "    * Often not the best option. There are many other ways to select source points. For example, many perspective transform algorithms will programmatically detect four source points in an image based on edge or corner detection and analyzing attributes like color and surrounding pixels.\n",
    "* Desired coordinates: chosen by eyeballing a rectangle\n",
    "\n",
    "\n",
    "`M = cv2.getPerspectiveTransform(src, dst)`\n",
    "* Returns mapping as a perspective matrix\n",
    "\n",
    "`warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)`\n",
    "* img: image to transform\n",
    "* M: Perspective matrix\n",
    "* img_size: size we want the warped image to be\n",
    "* flags: how to interpolate points, i.e. filling in points as it warps an image.\n",
    "* Returns warped image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/18-20.png)\n",
    "![](images/18-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copied from 'Transform a Stop Sign'\n",
    "\n",
    "Examples of Useful Code\n",
    "\n",
    "Compute the perspective transform, M, given source and destination points:\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "Compute the inverse perspective transform:\n",
    "\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "Warp an image using the perspective transform, M:\n",
    "\n",
    "warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "Note: When you apply a perspective transform, choosing four source points manually, as we did in this video, is often not the best option. There are many other ways to select source points. For example, many perspective transform algorithms will programmatically detect four source points in an image based on edge or corner detection and analyzing attributes like color and surrounding pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corners_unwarp(img, nx, ny, mtx, dist):\n",
    "    # Pass in your image into this function\n",
    "    # Write code to do the following steps\n",
    "    # 1) Undistort using mtx and dist\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # 2) Convert to grayscale\n",
    "    gray = cv2.cvtColor(undist, cv2.COLOR_BGR2GRAY)\n",
    "    # 3) Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    # 4) If corners found: \n",
    "    print(\"Corners: \", corners)\n",
    "    if corners.any() == True:\n",
    "            # a) draw corners\n",
    "            cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "            # b) define 4 source points src = np.float32([[,],[,],[,],[,]])\n",
    "                 #Note: you could pick any four of the detected corners \n",
    "                 # as long as those four corners define a rectangle\n",
    "                 # One especially smart way to do this would be to use four well-chosen\n",
    "                 # corners that were automatically detected during the undistortion steps\n",
    "                 # We recommend using the automatic detection of corners in your code\n",
    "            src = np.float32([[corners[0][0][0], corners[0][0][1]], \n",
    "                             [corners[nx-1][0][0], corners[nx-1][0][1]],\n",
    "                             [corners[nx*ny-1][0][0], corners[nx*ny-1][0][1]],\n",
    "                             [corners[nx*(ny-1)-1][0][0], corners[nx*(ny-1)-1][0][1]]])\n",
    "            print(\"Source points: \", src)\n",
    "            # c) define 4 destination points dst = np.float32([[,],[,],[,],[,]])\n",
    "            dst = np.float32([[0,0], [img.shape[0], 0], \n",
    "                              [img.shape[0], img.shape[1]], [0, img.shape[1]]])\n",
    "            print(\"Destination points: \", dst)\n",
    "            # d) use cv2.getPerspectiveTransform() to get M, the transform matrix\n",
    "            M = cv2.getPerspectiveTransform(src, dst)\n",
    "            # e) use cv2.warpPerspective() to warp your image to a top-down view\n",
    "            warped = cv2.warpPerspective(img, M, (gray.shape[0], gray.shape[1]), flags=cv2.INTER_LINEAR) \n",
    "    return warped, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns error `\n",
    "operands could not be broadcast together with shapes (960,1280,3) (1280,960,3) `"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Udacity's Answer\n",
    "\n",
    "# Define a function that takes an image, number of x and y points, \n",
    "# camera matrix and distortion coefficients\n",
    "def corners_unwarp(img, nx, ny, mtx, dist):\n",
    "    # Use the OpenCV undistort() function to remove distortion\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # Convert undistorted image to grayscale\n",
    "    gray = cv2.cvtColor(undist, cv2.COLOR_BGR2GRAY)\n",
    "    # Search for corners in the grayscaled image\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "    if ret == True:\n",
    "        # If we found corners, draw them! (just for fun)\n",
    "        cv2.drawChessboardCorners(undist, (nx, ny), corners, ret)\n",
    "        # Choose offset from image corners to plot detected corners\n",
    "        # This should be chosen to present the result at the proper aspect ratio\n",
    "        # My choice of 100 pixels is not exact, but close enough for our purpose here\n",
    "        offset = 100 # offset for dst points\n",
    "        # Grab the image shape\n",
    "        img_size = (gray.shape[1], gray.shape[0])\n",
    "\n",
    "        # For source points I'm grabbing the outer four detected corners\n",
    "        src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "        # For destination points, I'm arbitrarily choosing some points to be\n",
    "        # a nice fit for displaying our warped result \n",
    "        # again, not exact, but close enough for our purposes\n",
    "        dst = np.float32([[offset, offset], [img_size[0]-offset, offset], \n",
    "                                     [img_size[0]-offset, img_size[1]-offset], \n",
    "                                     [offset, img_size[1]-offset]])\n",
    "        # Given src and dst points, calculate the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "        # Warp the image using OpenCV warpPerspective()\n",
    "        warped = cv2.warpPerspective(undist, M, img_size)\n",
    "\n",
    "    # Return the resulting image and matrix\n",
    "    return warped, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Ideas from Udacity's Answer\n",
    "* Use an offset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Threshold\n",
    "\n",
    "Canny Edge Detection: Gave us many edges we ended up discarding.\n",
    "![](images/18-22.png)\n",
    "Lane lines: Looking for steeper lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobel Operator\n",
    "A way of taking the derivative of the image in the x or y direction.\n",
    "\n",
    "Sobel operators with a kernel size of 3 (i.e. 3x3 operator in each case):\n",
    "![](images/18-23.png)\n",
    "Kernels can be any odd number >= 3. A larger kernel implies taking the gradient over a larger region of the image, i.e. a smoother gradient.\n",
    "\n",
    "How they work:\n",
    "* Think of overlaying either one on a 3 x 3 region of an image. \n",
    "* If the image is flat across that region, the the result (summing the element-wise product of the operator and corresponding image pixels) will be zero. \n",
    "* If, instead, for example, you apply the S_x operator to a region of the image where values are rising from left to right, then the result will be positive, implying a positive derivative.\n",
    "\n",
    "![](images/18-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of Useful Code\n",
    "\n",
    "You need to pass a single color channel to the cv2.Sobel() function, so first convert to grayscale:\n",
    "\n",
    "```\n",
    "gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)```\n",
    "\n",
    "Note: Make sure you use the correct grayscale conversion depending on how you've read in your images. Use `cv2.COLOR_RGB2GRAY` if you've read in an image using `mpimg.imread()` or `cv2.COLOR_BGR2GRAY` if you've read in an image using `cv2.imread()`.\n",
    "\n",
    "Calculate the derivative in the x-direction (the 1, 0 at the end denotes x-direction):\n",
    "\n",
    "```\n",
    "sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)```\n",
    "\n",
    "Calculate the derivative in the y-direction (the 0, 1 at the end denotes y-direction):\n",
    "\n",
    "```\n",
    "sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1)```\n",
    "\n",
    "Calculate the absolute value of the x-derivative:\n",
    "\n",
    "```\n",
    "abs_sobelx = np.absolute(sobelx)```\n",
    "\n",
    "Convert the absolute value image to 8-bit:\n",
    "\n",
    "```\n",
    "scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))```\n",
    "\n",
    "Note: It's not entirely necessary to convert to 8-bit (range from 0 to 255) but in practice, it can be useful in the event that you've written a function to apply a particular threshold, and you want it to work the same on input images of different scales, like jpg vs. png. You could just as well choose a different standard range of values, like 0 to 1 etc.\n",
    "\n",
    "Create a binary threshold to select pixels based on gradient strength:\n",
    "\n",
    "```\n",
    "thresh_min = 20\n",
    "thresh_max = 100\n",
    "sxbinary = np.zeros_like(scaled_sobel)\n",
    "sxbinary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "plt.imshow(sxbinary, cmap='gray')\n",
    "```\n",
    "And here's what that result looks like:\n",
    "\n",
    "\n",
    "![](images/18-25.jpg)\n",
    "*Pixels have a value of 1 or 0 based on the strength of the x-gradient.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Read in an image and grayscale it\n",
    "image = mpimg.imread('signs_vehicles_xygrad.png')\n",
    "\n",
    "# Define a function that applies Sobel x or y, \n",
    "# then takes an absolute value and applies a threshold.\n",
    "# Note: calling your function with orient='x', thresh_min=5, thresh_max=100\n",
    "# should produce output like the example image shown above this quiz.\n",
    "def abs_sobel_thresh(img, orient='x', thresh_min=0, thresh_max=255):\n",
    "    \n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    if orient == 'x':\n",
    "        derivative = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "    elif orient == 'y':\n",
    "        derivative = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n",
    "    else:\n",
    "        print(\"Error: orient must be either x or y.\")\n",
    "    \n",
    "    # 3) Take the absolute value of the derivative or gradient\n",
    "    abs_derivative = np.absolute(derivative)\n",
    "    \n",
    "    # 4) Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_derivative/np.max(abs_derivative))\n",
    "\n",
    "    # 5) Create a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "    # So there are 1s where #s are within our thresholds and 0s otherwise.\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "    \n",
    "    # 6) Return this mask as your binary_output image\n",
    "    binary_output = sxbinary\n",
    "    return binary_output\n",
    "    \n",
    "# Run the function\n",
    "grad_binary = abs_sobel_thresh(image, orient='x', thresh_min=20, thresh_max=100)\n",
    "# Plot the result\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(grad_binary, cmap='gray')\n",
    "ax2.set_title('Thresholded Gradient', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "def abs_sobel_thresh(img, orient='x', thresh_min=0, thresh_max=255):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "![](images/18-26.png)\n",
    "\n",
    "**Magnitude of the Gradient**\n",
    "\n",
    "Next, apply a threshold to the overall magnitude of the gradient, in both x and y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Read in an image\n",
    "image = mpimg.imread('signs_vehicles_xygrad.png')\n",
    "\n",
    "# Define a function that applies Sobel x and y, \n",
    "# then computes the magnitude of the gradient\n",
    "# and applies a threshold\n",
    "def mag_thresh(img, sobel_kernel=9, mag_thresh=(0, 255)):\n",
    "    \n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n",
    "    \n",
    "    # 3) Calculate the magnitude \n",
    "    abs_sobelxy = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # 5) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_sobelxy/np.max(abs_sobelxy))\n",
    "    \n",
    "    # 6) Create a binary mask where mag thresholds are met\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= mag_thresh[0]) & (scaled_sobel <= mag_thresh[1])] = 1\n",
    "    \n",
    "    # 7) Return this mask as your binary_output image\n",
    "    return binary_output\n",
    "\n",
    "# Run the function\n",
    "mag_binary = mag_thresh(image, sobel_kernel=9, mag_thresh=(30, 100))\n",
    "# Plot the result\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(mag_binary, cmap='gray')\n",
    "ax2.set_title('Thresholded Magnitude', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "# Define a function to return the magnitude of the gradient\n",
    "# for a given sobel kernel size and threshold values\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "![](images/18-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Direction of the Gradient\n",
    "\n",
    "Write a function to compute the direction of the gradient and apply a threshold.\n",
    "\n",
    "Motivation: When looking for lane lines, we are only interested in edges that are nearly vertical.\n",
    "\n",
    "The direction of the gradient is simply the arctangent of the y-gradient divided by the x-gradient. `arctan(sobel_y/sobel_x)`. Each pixel of the resulting image contains a value for the angle of the gradient away from horizontal in units of radians, covering a range of −π/2 to π/2. An orientation of 0 implies a horizontal line and orientations of +/−π/2 imply vertical lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Read in an image\n",
    "image = mpimg.imread('signs_vehicles_xygrad.png')\n",
    "\n",
    "# Define a function that applies Sobel x and y, \n",
    "# then computes the direction of the gradient\n",
    "# and applies a threshold.\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    \n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    \n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient \n",
    "    abs_grad_dir = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    \n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    binary_output = np.zeros_like(abs_grad_dir)\n",
    "    binary_output[(abs_grad_dir >= thresh[0]) & (abs_grad_dir <= thresh[1])] = 1\n",
    "    \n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return binary_output\n",
    "    \n",
    "# Run the function\n",
    "dir_binary = dir_threshold(image, sobel_kernel=15, thresh=(0.7, 1.3))\n",
    "# Plot the result\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(dir_binary, cmap='gray')\n",
    "ax2.set_title('Thresholded Grad. Dir.', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: \n",
    "![](images/18-28.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
