{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robotics:\n",
    "1. Perception: Sense what is in the world (CV)\n",
    "    - Detect lane markings, vehicles.\n",
    "    - Alternative to cameras: radar + lidar (see the world in 3D, more costly, lower spatial resolution compared to cameras)\n",
    "        - Possible that in future cars are fitted with only a few cameras for perception as opposed to cameras, radars and lidars.\n",
    "2. Decide what to do based on that perception\n",
    "3. Perform action to carry out decision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two pieces:\n",
    "- Advanced lane lines\n",
    "- Vehicle (other vehicles) location and tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action: Steer car** \n",
    "- Steer car: Need to measure how much lane is curving. \n",
    "- Need to get perspective transformation from front view to birds-eye view.\n",
    "- Correct for effect of image distortion.\n",
    "    - changes shape and size of objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for image distortion\n",
    "\n",
    "### Types of distortion\n",
    "\n",
    "![](images/18-1.png)\n",
    "\n",
    "#### Pinhole camera model\n",
    "\n",
    "![](images/18-2.png)\n",
    "Image captured by pinhole camera is upside-down and reversed.\n",
    "\n",
    "Real cameras don't use pinholes - they use lenses. Lenses introduce distortion. Light rays bend too much or too little at the edges of a lens.\n",
    "\n",
    "![](images/18-4.png)\n",
    "\n",
    "\n",
    "**Radial distortion**: Real cameras use curved lenses to form an image, and light rays often bend a little too much or too little at the edges of these lenses. This creates an effect that distorts the edges of images, so that lines or objects appear more or less curved than they actually are. This is called radial distortion, and it’s the most common type of distortion.\n",
    "- Fisheye lenses use radial distortion for a stylistic effect.\n",
    "\n",
    "**Tangential distortion**: This occurs when a camera’s lens is not aligned perfectly parallel to the imaging plane, where the camera film or sensor is. This makes an image look tilted so that some objects appear farther away or closer than they actually are.\n",
    "\n",
    "\n",
    "### Distortion coefficients and correction\n",
    "Typically five coefficients\n",
    "\n",
    "![](images/18-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To undistort points, OpenCV calculates `r`, where \n",
    "$$r = ||(x_{corrected}, y_{corrected})-(x_c, y_c)||$$\n",
    "\n",
    "where $(x_c, y_c)$ is the *distortion center*, the center of the image distortion.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](images/18-5.png)\n",
    "*Points in an distorted and undistorted (corrected) image. The point (x, y) is a single point in a distorted image and (x_corrected, y_corrected) is where that point will appear in the undistorted (corrected) image.*\n",
    "\n",
    "**Radial distortion correction formula**: We can calculate `(x_corrected, y_corrected)` thus:\n",
    "![](images/18-6.png)\n",
    "*Radial distortion correction.*\n",
    "\n",
    "**Tangential distortion correction formula**:\n",
    "![](images/18-7.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrating our camera\n",
    "\n",
    "1. Take pictures of known shapes to calibrate our camera. E.g. a chessboard (a regular, high-contrast pattern).\n",
    "    * Recommended to use at least 20 images for calibration. Images taken at different angles and distances.\n",
    "    * Include a test image.\n",
    "\n",
    "2. Store the object and image points from each image.\n",
    "    * Object points: 3D points in real world space\n",
    "        * E.g. for a 7 row by 9 column chessboard:\n",
    "            * ![](images/18-9.png)\n",
    "        * Initialise arrays and fill arrays using objpo = np.zeros\n",
    "            `objp = np.zeros((6*8,3), np.float32)\n",
    "            objp[:,:2] = np.mgrid(8:8,0:6].T.reshape[-1,2]`\n",
    "    * Image points: 2D points in image plane\n",
    "3. Find chessboard corners\n",
    "\n",
    "2. Create a transform that maps distorted points to undistorted (corrected) points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Corners\n",
    "\n",
    "Use the OpenCV functions [findChessboardCorners()](http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.findChessboardCorners) and [drawChessboardCorners()](http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.drawChessboardCorners) to automatically find and draw corners in your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding Corners exercise\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import glob\n",
    "# prepare object points\n",
    "nx = 8 # Number of inside corners in any given row\n",
    "ny = 6 # Number of inside corners in any given column\n",
    "\n",
    "# Read in and make a list of calibration images\n",
    "\n",
    "# glob allows us to read in files with consistent file names\n",
    "# e.g. calibration-1.jpg, calibration-2.jpg...\n",
    "images = glob.glob(\"calibration_images/calibration*.jpg\")\n",
    "\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    # Parameters: (image, chessboard dims, param for any flags)\n",
    "    # chessboard dims = inside corners, not squares.\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "    # If found, draw corners\n",
    "    if ret == True:\n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calibration image provided with result for Finding Corners exercise:*\n",
    "![](images/18-8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV functions to calibrate our camera\n",
    "\n",
    "`cv2.calibrateCamera`:\n",
    "![](images/18-10.png)\n",
    "\n",
    "* `gray.shape[::-1]`: Shape of the image\n",
    "* `dist`: distortion coefficient\n",
    "* `mtx`: Camera matrix\n",
    "* `rvecs, tvecs`: position of camera in the world.\n",
    "\n",
    "![](images/18-4.png)\n",
    "*Camera matrix*\n",
    "\n",
    "`cv2.undistort(img, mtx, dist, None, mtx)`:\n",
    "\n",
    "* `img`: image\n",
    "* `mtx`: Camera matrix\n",
    "* `dist`: distortion coefficients\n",
    "* Returns undistorted (destination) image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just copying from 'Calibrating Your Camera'\n",
    "\n",
    "Examples of Useful Code\n",
    "\n",
    "Converting an image, imported by cv2 or the glob API, to grayscale:\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "Note: If you are reading in an image using mpimg.imread() this will read in an RGB image and you should convert to grayscale using cv2.COLOR_RGB2GRAY, but if you are using cv2.imread() or the glob API, as happens in this video example, this will read in a BGR image and you should convert to grayscale using cv2.COLOR_BGR2GRAY. We'll learn more about color conversions later on in this lesson, but please keep this in mind as you write your own code and look at code examples.\n",
    "\n",
    "Finding chessboard corners (for an 8x6 board):\n",
    "\n",
    "ret, corners = cv2.findChessboardCorners(gray, (8,6),None)\n",
    "Drawing detected corners on an image:\n",
    "\n",
    "img = cv2.drawChessboardCorners(img, (8,6), corners, ret)\n",
    "Camera calibration, given object points, image points, and the shape of the grayscale image:\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "Undistorting a test image:\n",
    "\n",
    "dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "A note on image shape\n",
    "The shape of the image, which is passed into the calibrateCamera function, is just the height and width of the image. One way to retrieve these values is by retrieving them from the grayscale image shape array gray.shape[::-1]. This returns the image height and width in pixel values like (960, 1280).\n",
    "\n",
    "Another way to retrieve the image shape, is to get them directly from the color image by retrieving the first two values in the color image shape array using img.shape[0:2]. This code snippet asks for just the first two values in the shape array.\n",
    "\n",
    "It's important to use an entire grayscale image shape or the first two values of a color image shape. This is because the entire shape of a color image will include a third value -- the number of color channels -- in addition to the height and width of the image. For example the shape array of a color image might be (960, 1280, 3), which are the pixel height and width of an image (960, 1280) and a third value (3) that represents the three color channels in the color image which you'll learn more about later, and if you try to pass these three values into the calibrateCamera function, you'll get an error."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Read in the saved objpoints and imgpoints\n",
    "dist_pickle = pickle.load( open( \"wide_dist_pickle.p\", \"rb\" ) )\n",
    "objpoints = dist_pickle[\"objpoints\"]\n",
    "imgpoints = dist_pickle[\"imgpoints\"]\n",
    "\n",
    "# Read in an image\n",
    "img = cv2.imread('test_image.png')\n",
    "\n",
    "# TODO: Write a function that takes an image, object points, and image points\n",
    "# performs the camera calibration, image distortion correction and \n",
    "# returns the undistorted image\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    # Use cv2.calibrateCamera and cv2.undistort()\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[0:2], None, None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "\n",
    "undistorted = cal_undistort(img, objpoints, imgpoints)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(undistorted)\n",
    "ax2.set_title('Undistorted Image', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting information from images of the road\n",
    "\n",
    "## Lane Curvature\n",
    "Why: Need to be told the correct steering angle to turn\n",
    "\n",
    "Process:\n",
    "1. Detect lane lines using masking and thresholding techniques.\n",
    "    * ![](images/18-11.png)\n",
    "2. Perform perspective transform to get birds' eye view of the lane.\n",
    "    * ![](images/18-12.png)\n",
    "\n",
    "3. Fit polynomial to lane line `f(y) = Ay^2 + By + C`.\n",
    "4. Extract curvature of lane lines using a mapping.\n",
    "    * ![](images/18-13.png)\n",
    "\n",
    "    * A gives you the curvature of the lane line, B gives you the heading or direction that the line is pointing, and C gives you the position of the line based on how far away it is from the very left of an image (y = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective transform\n",
    "Changes apparent persective. Maps the points in a given image to different, desired, image points with a new perspective.\n",
    "![](images/18-16.png)\n",
    "\n",
    "Examples of perspective:\n",
    "![](images/18-14.png)\n",
    "![](images/18-15.png)\n",
    "\n",
    "Birds eye view also allows us to map a car's location to e.g. Google Maps.\n",
    "\n",
    "![](images/18-17.png)\n",
    "![](images/18-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use traffic sign because it's easy to see if you've performed a perspective transform (well) when using a flat surface with distinct reference points e.g. text.\n",
    "Four points are enough to define a linear perspective transform. (Q: Why not three if it's a plane?)\n",
    "\n",
    "* Source coordinates: Manually find coordinates of points in an interactive window (mouse over points in the image).\n",
    "    * Often not the best option. There are many other ways to select source points. For example, many perspective transform algorithms will programmatically detect four source points in an image based on edge or corner detection and analyzing attributes like color and surrounding pixels.\n",
    "* Desired coordinates: chosen by eyeballing a rectangle\n",
    "\n",
    "\n",
    "`M = cv2.getPerspectiveTransform(src, dst)`\n",
    "* Returns mapping as a perspective matrix\n",
    "\n",
    "`warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)`\n",
    "* img: image to transform\n",
    "* M: Perspective matrix\n",
    "* img_size: size we want the warped image to be\n",
    "* flags: how to interpolate points, i.e. filling in points as it warps an image.\n",
    "* Returns warped image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copied from 'Transform a Stop Sign'\n",
    "\n",
    "Examples of Useful Code\n",
    "\n",
    "Compute the perspective transform, M, given source and destination points:\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "Compute the inverse perspective transform:\n",
    "\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "Warp an image using the perspective transform, M:\n",
    "\n",
    "warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "Note: When you apply a perspective transform, choosing four source points manually, as we did in this video, is often not the best option. There are many other ways to select source points. For example, many perspective transform algorithms will programmatically detect four source points in an image based on edge or corner detection and analyzing attributes like color and surrounding pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corners_unwarp(img, nx, ny, mtx, dist):\n",
    "    # Pass in your image into this function\n",
    "    # Write code to do the following steps\n",
    "    # 1) Undistort using mtx and dist\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # 2) Convert to grayscale\n",
    "    gray = cv2.cvtColor(undist, cv2.COLOR_BGR2GRAY)\n",
    "    # 3) Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    # 4) If corners found: \n",
    "    print(\"Corners: \", corners)\n",
    "    if corners.any() == True:\n",
    "            # a) draw corners\n",
    "            cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "            # b) define 4 source points src = np.float32([[,],[,],[,],[,]])\n",
    "                 #Note: you could pick any four of the detected corners \n",
    "                 # as long as those four corners define a rectangle\n",
    "                 # One especially smart way to do this would be to use four well-chosen\n",
    "                 # corners that were automatically detected during the undistortion steps\n",
    "                 # We recommend using the automatic detection of corners in your code\n",
    "            src = np.float32([[corners[0][0][0], corners[0][0][1]], \n",
    "                             [corners[nx-1][0][0], corners[nx-1][0][1]],\n",
    "                             [corners[nx*ny-1][0][0], corners[nx*ny-1][0][1]],\n",
    "                             [corners[nx*(ny-1)-1][0][0], corners[nx*(ny-1)-1][0][1]]])\n",
    "            print(\"Source points: \", src)\n",
    "            # c) define 4 destination points dst = np.float32([[,],[,],[,],[,]])\n",
    "            dst = np.float32([[0,0], [img.shape[0], 0], \n",
    "                              [img.shape[0], img.shape[1]], [0, img.shape[1]]])\n",
    "            print(\"Destination points: \", dst)\n",
    "            # d) use cv2.getPerspectiveTransform() to get M, the transform matrix\n",
    "            M = cv2.getPerspectiveTransform(src, dst)\n",
    "            # e) use cv2.warpPerspective() to warp your image to a top-down view\n",
    "            warped = cv2.warpPerspective(img, M, (gray.shape[0], gray.shape[1]), flags=cv2.INTER_LINEAR) \n",
    "    return warped, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns error `\n",
    "operands could not be broadcast together with shapes (960,1280,3) (1280,960,3) `"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Udacity's Answer\n",
    "\n",
    "# Define a function that takes an image, number of x and y points, \n",
    "# camera matrix and distortion coefficients\n",
    "def corners_unwarp(img, nx, ny, mtx, dist):\n",
    "    # Use the OpenCV undistort() function to remove distortion\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # Convert undistorted image to grayscale\n",
    "    gray = cv2.cvtColor(undist, cv2.COLOR_BGR2GRAY)\n",
    "    # Search for corners in the grayscaled image\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "    if ret == True:\n",
    "        # If we found corners, draw them! (just for fun)\n",
    "        cv2.drawChessboardCorners(undist, (nx, ny), corners, ret)\n",
    "        # Choose offset from image corners to plot detected corners\n",
    "        # This should be chosen to present the result at the proper aspect ratio\n",
    "        # My choice of 100 pixels is not exact, but close enough for our purpose here\n",
    "        offset = 100 # offset for dst points\n",
    "        # Grab the image shape\n",
    "        img_size = (gray.shape[1], gray.shape[0])\n",
    "\n",
    "        # For source points I'm grabbing the outer four detected corners\n",
    "        src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "        # For destination points, I'm arbitrarily choosing some points to be\n",
    "        # a nice fit for displaying our warped result \n",
    "        # again, not exact, but close enough for our purposes\n",
    "        dst = np.float32([[offset, offset], [img_size[0]-offset, offset], \n",
    "                                     [img_size[0]-offset, img_size[1]-offset], \n",
    "                                     [offset, img_size[1]-offset]])\n",
    "        # Given src and dst points, calculate the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "        # Warp the image using OpenCV warpPerspective()\n",
    "        warped = cv2.warpPerspective(undist, M, img_size)\n",
    "\n",
    "    # Return the resulting image and matrix\n",
    "    return warped, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Ideas from Udacity's Answer\n",
    "* Use an offset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Threshold\n",
    "\n",
    "Canny Edge Detection: Gave us many edges we ended up discarding.\n",
    "![]\n",
    "Lane lines: Looking for steeper lines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
